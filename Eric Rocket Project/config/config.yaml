behaviors:
  rocket landing:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512 # number of experiences used per training update
      buffer_size: 10240 # number of experiences stored in the replay buffer; the larger the buffer, the stable but slower the training
      learning_rate: 3.0e-4 # step size for updating the model parameters; the smaller the learning rate, the slower but more stable the training
      beta: 1.0e-2 # strength of the entropy regularization term; the larger the beta, the more exploration
      epsilon: 0.2 # Clipping parameter for PPO; controls how much the policy can change during an update
      lambd: 0.95 # (0-1) the higher, the less biased but more variable the estimate of the advantage function
      learning_rate_schedule: linear # learning rate schedule; options: constant, linear, exponential Note: decrease the learning rate over time to stabilize training
      num_epoch:    3 # number of times the training loop is run per update
    network_settings:
      normalize: false # normalize the input states
      hidden_units: 1024 # number of units in the hidden layers (originally 256)
      num_layers: 5 # number of hidden layers (originally 3)
      strength: 0.1 # (0-1) scales the reward signal; the larger the strength, the more the agent relies on the reward signal
    reward_signals:
      extrinsic:
        gamma: 0.99 # discount factor for future rewards
        strength: 1.0 # scales the reward signal; the larger the strength, the more the agent relies on the reward signal
        network_settings:
          normalize: false # normalize the input states
          hidden_units: 128 # number of units in the hidden layers
          num_layers: 2 # number of hidden layers
          strength: 0.1 # scales the reward signal; the larger the strength, the more the agent relies on the reward signal
      gail:
        strength: 0.1 # scales the reward signal; the larger the strength, the more the agent relies on the reward signal
        demo_path: "demo/LandingDemo.demo"
        network_settings:
          normalize: false # normalize the input states
          hidden_units: 128 # number of units in the hidden layers
          num_layers: 2 # number of hidden layers
          strength: 0.1 # scales the reward signal; the larger the strength, the more the agent relies on the reward signal
    behavioral_cloning:
      strength: 0.5 # scales the reward signal; the larger the strength, the more the agent relies on the reward signal
      demo_path: "demo/LandingDemo.demo"
    max_steps: 750000 # maximum number of steps in a given training session
    time_horizon: 128 # number of steps the agent considers before making a decision
    summary_freq: 20000 # how often to save training statistics

# default_settings: null
# behaviors:
#   rocket landing:
#     trainer_type: ppo
#     hyperparameters:
#       batch_size: 512
#       buffer_size: 10240
#       learning_rate: 0.0003
#       beta: 0.005
#       epsilon: 0.2
#       lambd: 0.95
#       num_epoch: 3
#       shared_critic: false
#       learning_rate_schedule: linear
#       beta_schedule: linear
#       epsilon_schedule: linear
#     checkpoint_interval: 500000
#     network_settings:
#       normalize: false
#       hidden_units: 512
#       num_layers: 3
#       vis_encode_type: simple
#       memory: null
#       goal_conditioning_type: hyper
#       deterministic: false
#     reward_signals:
#       extrinsic:
#         gamma: 0.99
#         strength: 1.0
#         network_settings:
#           normalize: false
#           hidden_units: 128
#           num_layers: 2
#           vis_encode_type: simple
#           memory: null
#           goal_conditioning_type: hyper
#           deterministic: false
#     init_path: null
#     keep_checkpoints: 5
#     even_checkpoints: false
#     max_steps: 740000
#     time_horizon: 64
#     summary_freq: 20000
#     threaded: false
#     self_play: null
#     behavioral_cloning: null
# env_settings:
#   env_path: null
#   env_args: null
#   base_port: 5005
#   num_envs: 1
#   num_areas: 1
#   timeout_wait: 60
#   seed: -1
#   max_lifetime_restarts: 10
#   restarts_rate_limit_n: 1
#   restarts_rate_limit_period_s: 60
# engine_settings:
#   width: 84
#   height: 84
#   quality_level: 5
#   time_scale: 20
#   target_frame_rate: -1
#   capture_frame_rate: 60
#   no_graphics: false
#   no_graphics_monitor: false
# environment_parameters: null
# checkpoint_settings:
#   run_id: TEST2
#   initialize_from: null
#   load_model: false
#   resume: false
#   force: true
#   train_model: false
#   inference: false
#   results_dir: results
# torch_settings:
#   device: null
# debug: false
